---
tags:
  - Foundation
  - Text
todo: false
summary: Mixture of Experts, kind of like ensemble method.
---
# Summary
ðŸ’¡ Write a brief summary of this paper here
[[MoE]] is short for Mixture of Experts.
By replacing FFN layers with [[MoE Layer]] and a gating network, the model can be pretrained with far less compute.
See more at this [hugging face post](https://huggingface.co/blog/moe).
# Resources
ðŸ’¡ Include some useful links or related papers for better understanding of this concept



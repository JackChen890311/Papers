---
title: "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"
time: 2311
author: Institute for Intelligent Computing, Alibaba Group
link: https://arxiv.org/pdf/2311.17117.pdf
accepted: None
tags:
  - Image
  - Video
  - Multimodal
  - HumanPose
  - Generation
todo: false
scanned: true
read: true
summary: A human animation model, based on stable diffusion and temporal layer, conditioned on reference image and motion sequence. Pose sequence is directly used as input of diffusion model along with noises.
---
# Summary
ðŸ’¡ Write a brief summary of this paper here
![[Pasted image 20240116180946.png]]
# Methodology
ðŸ’¡ Describe the methodology used in this paper

# Experiments
ðŸ’¡ List the experiments settings and results of this paper

# Related Papers
ðŸ’¡ Include any related papers that are relevant to this one
Different from [[MagicAnimate]] and [[DreaMoving]], they put human pose skelenton as the input to [[Diffusion Models]].
Use [[Latent Diffusion]] as base model.

# Appendix
ðŸ’¡ Anything else thatâ€™s in this paper but not metioned before

---
# Resources
ðŸ’¡ Include some useful links for better understanding of this paper
- [Project Page](https://humanaigc.github.io/animate-anyone/)
- [Unofficial Reproduce](https://github.com/MooreThreads/Moore-AnimateAnyone)
- [Human Animation](https://docs.google.com/presentation/d/17OTLGDjjYoZJA6TdNU4ecOPky7Xja3MViUpTYSfNXjM/edit#slide=id.g2a7f2eb90a5_0_61)
# Personal Notes
ðŸ’¡ Personal thoughts, reflections, or questions about this paper
---
title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
time: 1810
author: Google AI Language
link: https://arxiv.org/pdf/1810.04805.pdf
accepted: None
tags:
  - Foundation
  - Text
todo: false
scanned: true
read: false
summary: A foundation language model, is very useful for downstream tasks
---
# Summary
ðŸ’¡ Write a brief summary of this paper here
![[Pasted image 20240414171421.png]]
![[Pasted image 20240414171623.png]]
# Methodology
ðŸ’¡ Describe the methodology used in this paper

# Experiments
ðŸ’¡ List the experiments settings and results of this paper

# Related Papers
ðŸ’¡ Include any related papers that are relevant to this one

# Appendix
ðŸ’¡ Anything else thatâ€™s in this paper but not metioned before

---
# Resources
ðŸ’¡ Include some useful links for better understanding of this paper
- [PWC](https://paperswithcode.com/method/bert)

# Personal Notes
ðŸ’¡ Personal thoughts, reflections, or questions about this paper
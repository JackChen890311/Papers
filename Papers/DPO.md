---
title: "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
time: 2305
author: Stanford University; CZ Biohub
link: https://arxiv.org/pdf/2305.18290
accepted: NeurIPS23
tags:
  - LLM
  - Loss
  - Text
  - Theory
todo: true
scanned: false
read: false
summary:
---
# Summary
ðŸ’¡ Write a brief summary of this paper here
![[Pasted image 20240520205822.png]]
# Methodology
ðŸ’¡ Describe the methodology used in this paper

# Experiments
ðŸ’¡ List the experiments settings and results of this paper

# Related Papers
ðŸ’¡ Include any related papers that are relevant to this one

# Appendix
ðŸ’¡ Anything else thatâ€™s in this paper but not metioned before

---
# Resources
ðŸ’¡ Include some useful links for better understanding of this paper
- [DPO Sharing Slides](https://docs.google.com/presentation/d/1LuOkbz_82AAX8sLrKGq2EMCL_ZB8pIkjWrMdLnH6uho/edit#slide=id.g2dad079529e_0_92)
- [è«–æ–‡ä»‹ç´¹ - DPO](https://datasciocean.tech/paper-intro/direct-preference-optimization)
- [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://arxiv.org/pdf/2404.10719)
# Personal Notes
ðŸ’¡ Personal thoughts, reflections, or questions about this paper
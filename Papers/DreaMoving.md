---
title: "DreaMoving: A Human Video Generation Framework based on Diffusion Models"
time: 2312
author: Alibaba Group
link: https://arxiv.org/pdf/2312.05107.pdf
accepted: None
tags:
  - Image
  - Text
  - Video
  - Multimodal
  - HumanPose
  - Generation
  - Diffusion
todo: false
scanned: true
read: true
summary: A human animation model, based on stable diffusion, control net and motion block, conditioned on reference image and motion sequence. A content guider is used to take in different modalities of conditions.
---
# Summary
ðŸ’¡ Write a brief summary of this paper here
![[Pasted image 20240116181257.png]]

# Methodology
ðŸ’¡ Describe the methodology used in this paper

# Experiments
ðŸ’¡ List the experiments settings and results of this paper

# Related Papers
ðŸ’¡ Include any related papers that are relevant to this one
Their concept is very similar with [[MagicAnimate]].
Use [[Latent Diffusion]] as base model, adapt the idea of motion block from [[AnimateDiff]].
Use [[ControlNet]] to take pose/depth as guidance.
# Appendix
ðŸ’¡ Anything else thatâ€™s in this paper but not metioned before

---
# Resources
ðŸ’¡ Include some useful links for better understanding of this paper
- [Project Page](https://dreamoving.github.io/dreamoving/)
- [Human Animation](https://docs.google.com/presentation/d/17OTLGDjjYoZJA6TdNU4ecOPky7Xja3MViUpTYSfNXjM/edit#slide=id.g2a7f2eb90a5_0_61)

# Personal Notes
ðŸ’¡ Personal thoughts, reflections, or questions about this paper
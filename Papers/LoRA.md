---
title: "LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS"
time: 2106
author: Microsoft Corporation
link: https://arxiv.org/pdf/2106.09685
accepted: None
tags:
  - Add-on
  - Foundation
  - LLM
  - Text
  - PEFT
todo: false
scanned: true
read: false
summary: A low rank method for adding a adaptation (fine-tuning) for existing model.
---
# Summary
ðŸ’¡ Write a brief summary of this paper here
A classical [[PEFT]] Method. Using low-rank matrix to add on original pretrained weight.
![[Pasted image 20250108164529.png]]
# Methodology
ðŸ’¡ Describe the methodology used in this paper

# Experiments
ðŸ’¡ List the experiments settings and results of this paper

# Related Papers
ðŸ’¡ Include any related papers that are relevant to this one

# Appendix
ðŸ’¡ Anything else thatâ€™s in this paper but not mentioned before

---
# Resources
ðŸ’¡ Include some useful links for better understanding of this paper

# Personal Notes
ðŸ’¡ Personal thoughts, reflections, or questions about this paper
---
title: "Lumiere: A Space-Time Diffusion Model for Video Generation"
time: 2401
author: Google Research; Weizmann Institute; Tel-Aviv University; Technion
link: https://arxiv.org/pdf/2401.12945.pdf
accepted: None
tags:
  - Video
  - Text
  - Multimodal
  - Generation
  - Diffusion
todo: true
scanned: false
read: false
summary: Use a 3d UNet (STUNet) to diffuse video frames in both temporal and spatial dimensions
---
# Summary
ðŸ’¡ Write a brief summary of this paper here
![[Pasted image 20240210121725.png]]

# Methodology
ðŸ’¡ Describe the methodology used in this paper
- Difference with previous works
	- Some techniques: SSR, [[MultiDiffusion]]
![[Pasted image 20240210121612.png]]
# Experiments
ðŸ’¡ List the experiments settings and results of this paper

# Related Papers
ðŸ’¡ Include any related papers that are relevant to this one
- 3D UNet is also used in [[AnimateDiff]] and related human pose works: [[Animate Anyone]], [[MagicAnimate]], [[DreaMoving]]
- Previous works: [VideoLDM](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)
- Related work: [ImaGen](https://imagen.research.google/)
# Appendix
ðŸ’¡ Anything else thatâ€™s in this paper but not metioned before

---
# Resources
ðŸ’¡ Include some useful links for better understanding of this paper
- [Demo Page](https://lumiere-video.github.io/)
- [Youtube è«–æ–‡å°Žè®€](https://www.youtube.com/watch?v=reyUJoWS2i8)

# Personal Notes
ðŸ’¡ Personal thoughts, reflections, or questions about this paper
---
title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
time: 1910
author: Google
link: https://arxiv.org/pdf/1910.10683.pdf
accepted: JMLR20
tags:
  - Foundation
  - Text
todo: false
scanned: true
read: false
summary: A foundation language model which treat all NLP tasks as generation task with a prefix
---
# Summary
ğŸ’¡ Write a brief summary of this paper here
T5 is a [[Transformer]]-based NLP model pre-trained on Colossal Clean Crawled Corpus (C4) dataset. 
T5 frames all the NLP tasks as text generation tasks, by adding a prefix like â€œsummarizeâ€ or â€œtranslate English to Germanâ€, T5 will know which task we want to perform and generate the corresponding results.
![[Pasted image 20240414172156.png]]
# Methodology
ğŸ’¡ Describe the methodology used in this paper

# Experiments
ğŸ’¡ List the experiments settings and results of this paper

# Related Papers
ğŸ’¡ Include any related papers that are relevant to this one

# Appendix
ğŸ’¡ Anything else thatâ€™s in this paper but not metioned before

---
# Resources
ğŸ’¡ Include some useful links for better understanding of this paper
- [T5 Note](https://hackmd.io/@_E6GATP9Sz2h9WVqOqMDbg/S1kFz_Xrc)

# Personal Notes
ğŸ’¡ Personal thoughts, reflections, or questions about this paper
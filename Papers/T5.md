---
title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
time: 1910
author: Google
link: https://arxiv.org/pdf/1910.10683.pdf
accepted: JMLR20
tags:
  - Foundation
  - Text
todo: false
scanned: true
read: false
summary: A foundation language model which treat all NLP tasks as generation task with a prefix
---
# Summary
💡 Write a brief summary of this paper here
T5 is a [[Transformer]]-based NLP model pre-trained on Colossal Clean Crawled Corpus (C4) dataset. 
T5 frames all the NLP tasks as text generation tasks, by adding a prefix like “summarize” or “translate English to German”, T5 will know which task we want to perform and generate the corresponding results.
![[Pasted image 20240414172156.png]]
# Methodology
💡 Describe the methodology used in this paper

# Experiments
💡 List the experiments settings and results of this paper

# Related Papers
💡 Include any related papers that are relevant to this one

# Appendix
💡 Anything else that’s in this paper but not metioned before

---
# Resources
💡 Include some useful links for better understanding of this paper
- [T5 Note](https://hackmd.io/@_E6GATP9Sz2h9WVqOqMDbg/S1kFz_Xrc)

# Personal Notes
💡 Personal thoughts, reflections, or questions about this paper